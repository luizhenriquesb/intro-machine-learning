---
title: "Introdução ao Machine Learning - Curso-R"
subtitle: "Aula 3: Cross-validation"
format: html
editor: visual
---

#### Recapitulação

-   Uma das maneiras de se proteger do overfitting é separar a base em duas partes, uma de treino e outra de teste

-   Na **base de Treino (dado observado),** usamos uma **função de custo** para escolher **parâmetros**

-   Na **base de Teste (simulação de dado novo)**, usamos **métricas** para escolher **hiperparâmetros**.

-   A diferença entre parâmetro e hiperparâmetro é importante

-   "Quanto mais complexo for o modelo, menor será o erro de *treino*. Porém, o que importa é o erro de *teste*"

-   Ao receber a base, a primeira coisa a ser feita é dividi-la em duas partes: Treino e Teste. Não mexeremos na base de Teste. Na base de Treino, vamos também separá-la em várias "partes de teste", que são chamadas de bases de Validação.

Função `initial_split(dados, prop=3/4)` separa entre dados antigos e novos (feita no começo da análise).

## Estratégias

### 1) Separar inicialmente a base de dados em duas: treino e teste

```{r, results='hide', echo=FALSE}
initial_split(dados, prop=3/4)      # 3/4 de treino aleatoriamente
initial_time_split(dados, prop=3/4) # 3/4 de treino respeitando a ordem
```

Fazemos esse procedimento para se proteger de data leakage ou vazamento de informação, que consiste na contaminação da base de treino pela base de teste.

### Regularização

Objetivo: oferecer um parâmetro (um valor que podemos mudar) para termos controle sobre a complexidade da $f(x)$ e assim evitar o overfitting.

A regularização adiciona uma penalidade à função de custo do modelo para evitar que os coeficientes dos parâmetros do modelo se tornem muito grandes.

No exemplo da regressão linear, haverá um valor ${\lambda}$ que chamaremos de "hiperparâmetro" da regressão. Iremos chutar diferentes valores de ${\lambda}$ até encontrar a melhor $f(x)$.

### Complexidade das regressões

Por que um grau 7 de polinômio é mais complexo do que um grau 2? Um dos motivos é que o grau 2 está contido no grau 7.

Outra noção de complexidade é que um modelo com grau 7 de polinômio tem mais variáveis em comparação com um modelo de grau 2, por exemplo.

### Regularização - LASSO

Intuição: para um beta qualquer $\beta_j$ entrar na regressão ele precisará diminuir o erro em uma certa quantidade $\lambda$. Se ele diminuir menos que $\lambda$ esse beta $\beta$ será jogado fora.

**ChatGTP:** Na L1 Regularization, também conhecida como Lasso (Least Absolute Shrinkage and Selection Operator), é adicionada à função de custo uma penalidade proporcional ao valor absoluto dos coeficientes dos parâmetros do modelo. Isso leva à redução de alguns coeficientes a zero, o que ajuda na seleção de características e simplificação do modelo.

Um $RMSE_{regularizado}$ é uma regressão que não deixamos ela crescer como ela "gostaria". Isso acontece ao deixarmos apenas os beta $\beta$ que reduzem o erro em certa quantidade $\lambda$.

Uma dificuldade que aparece está relacionada à comparação de diversos betas $\beta$ de uma só vez. Isso acontece porque podemos ter um beta $\beta$ muito grande, de modo que facilmente ele poderia diminuir o erro abaixo de $\lambda$. Como lidar com isso? Devemos deixar todos os betas na mesma escala. Precisamos transformar os $x$ para deixá-los todos na mesma escala.

Esse método funciona como seleção de variáveis porque obriga que beta $\beta$ seja minimamente importante para entrar no modelo. Ou sejaa, penalizamos a função de custo se os beta $\beta$ forem muito grandes

-   O $\lambda$ é um hiperparâmetro da regressão linear

-   Quanto maior o $\lambda$, mais penalizamos os $\beta$ por serem grandes

### Hiperparâmetros

São parâmetros que têm que ser definidos antes de ajustar o modelo. Não há como achar o valor ótimo diretamente nas funções de custo. Precisam ser achados na força bruta.

```{r, results='hide', echo=FALSE}
linear_reg(penalty = 0.0) # sem regularização
linear_reg(penalty = 0.1)
linear_reg(penalty = 1.0)
linear_reg(penalty = tune())
```

O argumento `penalty = tune()` calcula a perda da métrica de erro para cada hiperparâmetro.

-   Queremos estimar o erro de predição (que não temos como conhecer, porque se refere ao futuro)

-   O erro de teste (calculado na base de Teste) é a melhor estimativa para o erro de predição

-   O erro de validação é utilizado para definir os hiperparâmetros. Fazer usar a técnica de cross-validation

## Cross-validation

A Validação cruzada estima muito bem o erro de predição. Seu objetivo é encontrar o melhor conjunto de hiperparâmetros

**Estratégia**

-   Dividir o banco de dados em K partes

-   Ajustar o modelo K vezes, sempre deixando um pedaço da base de fora para servir como base de teste

-   Teremos K valores de erros de teste. Tira-se a média dos erros

**Erro de validação cruzada**

$$
RMSE_{cv}=\frac{1}{5}\sum_{i=1}^{5}RMSE_{Fold_i}  
$$

O máximo de *folds* que podemos fazer é $n-1$, em que $n = {número\ de\ dados}$. É comum fazer $n = 5$.

**Em pseudo-código**

```{r, echo=FALSE}
k <- 5

fold <- sample.int(k, nrow(mtcars), replace = TRUE)

for (k in 1:k) {
  train <- mtcars[fold != k,]
  valid <- mtcars[fold == k,]
  
  # ajusta modelo (train)
  # metric(valid)
}
```

Esse tipo de código é comum no sklearn (Python). No tidymodels, temos funções que fazem isso. Existem outros pacotes que fazem a mesma coisa, a diferença está no grau de controle sobre cada procedimento.

1h44
